{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e137dd-5d70-4bac-a5ac-0c8a17352e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.1.0)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pyspark) (0.10.9.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/28 14:35:36 WARN Utils: Your hostname, DIVAs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "25/12/28 14:35:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/28 14:35:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CustomerOrdersJob\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2519237e-6580-43f4-bd6a-97da91b2f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_path = \"/Users/divamodi/pyspark-project/spark-apps/test_input/customers.csv\"\n",
    "orders_path = \"/Users/divamodi/pyspark-project/spark-apps/test_input/orders.json\"\n",
    "output_path_csv = \"/Users/divamodi/pyspark-project/spark-apps/output/tmp/orders_enriched_csv\"\n",
    "output_path_parquet = \"/Users/divamodi/pyspark-project/spark-apps/output/tmp/orders_enriched_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a260b03c-102a-4bd9-8ec1-6856ee25b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = spark.read.option(\"header\", True).csv(customers_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00a98496-dba9-4a14-8ad4-9392fc07d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = spark.read.json(orders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc187146-e7c7-4914-ac72-c9a099b22138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+----------+\n",
      "|amount|customer_id|order_id|    status|\n",
      "+------+-----------+--------+----------+\n",
      "| 250.5|        101|    5001|   shipped|\n",
      "| 145.0|        102|    5002| cancelled|\n",
      "|389.99|        104|    5003|processing|\n",
      "| 89.99|        105|    5004|   shipped|\n",
      "| 199.0|        106|    5005|  returned|\n",
      "+------+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db92bf24-039c-45df-a8b4-35b6d9f42b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/divamodi/pyspark-project/notebooks/pyspark/practice\n",
      "notebook dir guess: /Users/divamodi/pyspark-project/notebooks/pyspark/practice\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"notebook dir guess:\", pathlib.Path().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344592d7-bc01-4f88-b0b0-7b0bb7af4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------------+\n",
      "|order_id|      name|amount|  order_type|\n",
      "+--------+----------+------+------------+\n",
      "|    5001|  John Doe| 250.5|  High Value|\n",
      "|    5002|Jane Smith| 145.0|Medium Value|\n",
      "|    5003|Rita Mehra|389.99|  High Value|\n",
      "|    5004| Wei Zhang| 89.99|   Low Value|\n",
      "+--------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = df_orders.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "df_enriched = df_joined.withColumn(\n",
    "    \"order_type\",\n",
    "    when(df_joined.amount >= 200, \"High Value\")\n",
    "    .when(df_joined.amount >= 100, \"Medium Value\")\n",
    "    .otherwise(\"Low Value\")\n",
    ")\n",
    "\n",
    "df_enriched.select(\"order_id\", \"name\", \"amount\", \"order_type\").show()\n",
    "\n",
    "df_enriched_op = df_enriched.select(\"order_id\", \"name\", \"amount\", \"order_type\")\n",
    "df_enriched_op.write.mode(\"overwrite\").option(\"header\", True).csv(output_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e820b9-427b-4fec-a108-33979b849fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/divamodi/pyspark-project/spark-apps/output/tmp/orders_enriched_csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1428e45a-5bf9-4db7-8bd9-73c0b91fb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_op.write.mode(\"overwrite\").parquet(output_path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d981baa0-c363-4acf-8769-51c6dfb8e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.createOrReplaceTempView('customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3e58db-7852-43a3-b69a-2f7b428fdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------------+---+-------+\n",
      "|customer_id|      name|           email|age|country|\n",
      "+-----------+----------+----------------+---+-------+\n",
      "|        101|  John Doe|john@example.com| 28|     US|\n",
      "|        102|Jane Smith|jane@example.com| 34|     UK|\n",
      "|        103|  Ali Khan| ali@example.com| 40|    UAE|\n",
      "|        104|Rita Mehra|rita@example.com| 25|  India|\n",
      "|        105| Wei Zhang| wei@example.com| 31|  China|\n",
      "+-----------+----------+----------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * from customers\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f476734-9a4f-4bc0-99bd-28278fae378e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
